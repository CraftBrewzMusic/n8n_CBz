version: '3.8'

services:
  n8n-cbz:
    build: .
    image: n8n-cbz-custom:1.120.0
    container_name: n8n-cbz
    restart: unless-stopped
    ports:
      - "5678:5678"
    environment:
      - N8N_HOST=0.0.0.0
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - WEBHOOK_URL=https://n8n.craftbrewzmusic.com/
      - WEBHOOK_TUNNEL_URL=https://n8n.craftbrewzmusic.com/
      - N8N_EDITOR_BASE_URL=https://n8n.craftbrewzmusic.com/
      - GENERIC_TIMEZONE=America/Denver 
      - TZ=America/Denver
      - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=true
      - N8N_RUNNERS_ENABLED=true
      # Misc
      - N8N_COMMUNITY_PACKAGE_ALLOW_TOOL_USAGE=true
      - N8N_DEFAULT_BINARY_DATA_MODE=filesystem
      - N8N_DEFAULT_CORS=true
      # Allow external npm packages in code nodes
      - NODE_FUNCTION_ALLOW_EXTERNAL=jsonata
    volumes:
      - n8n_data:/home/node/.n8n
      # Mount local folder for custom nodes (optional)
      # - ./custom-nodes:/home/node/.n8n/custom
    extra_hosts:
      # This allows n8n to reach your host machine where LM Studio/Ollama runs
      - "host.docker.internal:host-gateway"

  cloudflared_n8n-cbz:
    image: cloudflare/cloudflared:latest
    restart: unless-stopped
    command: tunnel --no-autoupdate run
    environment:
      - TUNNEL_TOKEN=eyJhIjoiYWJlMzYwNThiZTJlMmJlNDI2MmJmYWFmZWI3YWY0M2UiLCJ0IjoiZWUyOTJhMTgtYTRiYi00NjQxLWFkZWQtNmViNmRiNmJkYzIzIiwicyI6Ik0yRTFORFZtT1RrdE5UQTNZeTAwWWpBd0xXRTJPRFF0TkRZelpERXpZemRtWWpNeCJ9
    depends_on:
      - n8n-cbz

volumes:
  n8n_data:
    driver: local

   